{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一、数据处理"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_data(root_path, m_type):\n",
    "    content_list = []\n",
    "    for i in range(1,6):\n",
    "        path = os.path.join(root_path, \"enron%d/%s/\" % (i, m_type))\n",
    "\n",
    "        file_list = os.listdir(path)\n",
    "        for file_name in file_list:\n",
    "            file_path = os.path.join(path, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                with open(file_path, encoding='utf-8', errors='ignore') as f:\n",
    "                    content = ''.join([line.strip() for line in f.readlines()])\n",
    "                content_list.append(content)\n",
    "    return content_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "非垃圾邮件数： 7522\n",
      "垃圾邮件数： 6336\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(99)\n",
    "sample_frac = 0.5\n",
    "ham = load_data('./data', 'ham')\n",
    "ham = random.sample(ham, round(sample_frac * len(ham)))\n",
    "spam = load_data('./data', 'spam')\n",
    "spam = random.sample(spam, round(sample_frac * len(spam)))\n",
    "print(\"非垃圾邮件数：\", len(ham))\n",
    "print(\"垃圾邮件数：\", len(spam))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 抽取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tflearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_document_length = 100\n",
    "def  get_features_by_tf(X, y):\n",
    "    vp = tflearn.data_utils.VocabularyProcessor(\n",
    "        max_document_length=max_document_length,\n",
    "        min_frequency=0,\n",
    "        vocabulary=None,\n",
    "        tokenizer_fn=None)\n",
    "    X = vp.fit_transform(X, unused_y=None)\n",
    "    X = np.array(list(X))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_by_wordbag_tfidf(X, y):\n",
    "    vectorizer = CountVectorizer(binary=False,\n",
    "                                 decode_error='ignore',\n",
    "                                 strip_accents='ascii',\n",
    "                                 max_features=10000,\n",
    "                                 stop_words='english',\n",
    "                                 max_df=1.0,\n",
    "                                 min_df=1 )\n",
    "    X = vectorizer.fit_transform(X)\n",
    "    X = X.toarray()\n",
    "    transformer = TfidfTransformer(smooth_idf=False)\n",
    "    tfidf = transformer.fit_transform(X)\n",
    "    X = tfidf.toarray()\n",
    "    return  X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集: 9700\n",
      "测试集: 4158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = ham + spam\n",
    "y=[0] * len(ham) + [1] * len(spam)\n",
    "proX, proy = get_features_by_wordbag_tfidf(X, y)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(proX, proy, test_size = 0.3, random_state = 99)\n",
    "print(\"训练集:\", len(ytrain))\n",
    "print(\"测试集:\", len(ytest))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 二、模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def evaluate(ytest, ypred):\n",
    "    print(\"accuracy:\\t\", metrics.accuracy_score(ytest, ypred))\n",
    "    print(\"precision:\\t\", metrics.precision_score(ytest, ypred))\n",
    "    print(\"recall:\\t\", metrics.recall_score(ytest, ypred))\n",
    "    print(\"confusion matrix:\\n\", metrics.confusion_matrix(ytest, ypred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集：\n",
      "accuracy:\t 0.9956701030927835\n",
      "precision:\t 0.9934611048478016\n",
      "recall:\t 0.9970581579542883\n",
      "confusion matrix:\n",
      " [[5252   29]\n",
      " [  13 4406]]\n"
     ]
    }
   ],
   "source": [
    "print(\"训练集：\")\n",
    "evaluate(ytrain, gnb.predict(Xtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集\n",
      "accuracy:\t 0.9677729677729677\n",
      "precision:\t 0.967488201363398\n",
      "recall:\t 0.9624413145539906\n",
      "confusion matrix:\n",
      " [[2179   62]\n",
      " [  72 1845]]\n"
     ]
    }
   ],
   "source": [
    "print(\"测试集\")\n",
    "evaluate(ytest, gnb.predict(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 支持向量机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm_model = svm.SVC()\n",
    "svm_model.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集：\n",
      "accuracy:\t 0.9989690721649485\n",
      "precision:\t 0.9981920903954802\n",
      "recall:\t 0.9995474089160443\n",
      "confusion matrix:\n",
      " [[5273    8]\n",
      " [   2 4417]]\n"
     ]
    }
   ],
   "source": [
    "print(\"训练集：\")\n",
    "evaluate(ytrain, svm_model.predict(Xtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集\n",
      "accuracy:\t 0.9843674843674843\n",
      "precision:\t 0.9705284552845529\n",
      "recall:\t 0.9963484611371936\n",
      "confusion matrix:\n",
      " [[2183   58]\n",
      " [   7 1910]]\n"
     ]
    }
   ],
   "source": [
    "print(\"测试集\")\n",
    "evaluate(ytest, svm_model.predict(Xtest))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(64, 32, 16), random_state=99)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(alpha=1e-5,\n",
    "                    hidden_layer_sizes = (64, 32, 16),\n",
    "                    random_state = 99)\n",
    "mlp.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集：\n",
      "accuracy:\t 0.9996907216494846\n",
      "precision:\t 0.9993215739484396\n",
      "recall:\t 1.0\n",
      "confusion matrix:\n",
      " [[5278    3]\n",
      " [   0 4419]]\n"
     ]
    }
   ],
   "source": [
    "print(\"训练集：\")\n",
    "evaluate(ytrain, mlp.predict(Xtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集\n",
      "accuracy:\t 0.9850889850889851\n",
      "precision:\t 0.9833246482542991\n",
      "recall:\t 0.9843505477308294\n",
      "confusion matrix:\n",
      " [[2209   32]\n",
      " [  30 1887]]\n"
     ]
    }
   ],
   "source": [
    "print(\"测试集\")\n",
    "evaluate(ytest, mlp.predict(Xtest))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.02518\u001b[0m\u001b[0m | time: 103.237s\n",
      "| Adam | epoch: 005 | loss: 0.02518 - acc: 0.9976 -- iter: 9600/9700\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.02415\u001b[0m\u001b[0m | time: 106.247s\n",
      "| Adam | epoch: 005 | loss: 0.02415 - acc: 0.9979 | val_loss: 0.07756 - val_acc: 0.9735 -- iter: 9700/9700\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from tflearn.layers.core import input_data, fully_connected, dropout\n",
    "from tflearn.layers.conv import conv_1d, global_max_pool\n",
    "from tflearn.layers.merge_ops import merge\n",
    "import tensorflow as tf\n",
    "proX, proy = get_features_by_tf(X, y)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(proX, proy, test_size = 0.3, random_state = 99)\n",
    "# Converting labels to binary vectors\n",
    "y_train = to_categorical(ytrain, num_classes=2)\n",
    "y_test = to_categorical(ytest, num_classes=2)\n",
    "\n",
    "# Building convolutional network\n",
    "network = input_data(shape=[None,max_document_length], name='input')\n",
    "network = tflearn.embedding(network, input_dim=1000000, output_dim=128)\n",
    "branch1 = conv_1d(network, 128, 3, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "branch2 = conv_1d(network, 128, 4, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "branch3 = conv_1d(network, 128, 5, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "network = merge([branch1, branch2, branch3], mode='concat', axis=1)\n",
    "network = tf.expand_dims(network, 2)\n",
    "network = global_max_pool(network)\n",
    "network = dropout(network, 0.8)\n",
    "network = fully_connected(network, 2, activation='softmax')\n",
    "network = tflearn.regression(network, optimizer='adam', learning_rate=0.001,\n",
    "                        loss='categorical_crossentropy', name='target')\n",
    "# Training\n",
    "model = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "model.fit(Xtrain, y_train,\n",
    "            n_epoch=5, shuffle=True, validation_set=(Xtest, y_test),\n",
    "            show_metric=True, batch_size=128,run_id=\"spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集：\n",
      "accuracy:\t 0.9997938144329896\n",
      "precision:\t 0.9995476136620675\n",
      "recall:\t 1.0\n",
      "confusion matrix:\n",
      " [[5279    2]\n",
      " [   0 4419]]\n"
     ]
    }
   ],
   "source": [
    "print(\"训练集：\")\n",
    "evaluate(ytrain, np.argmax(model.predict(Xtrain), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集：\n",
      "accuracy:\t 0.9735449735449735\n",
      "precision:\t 0.9683773976153447\n",
      "recall:\t 0.9744392279603548\n",
      "confusion matrix:\n",
      " [[2180   61]\n",
      " [  49 1868]]\n"
     ]
    }
   ],
   "source": [
    "print(\"测试集：\")\n",
    "evaluate(ytest, np.argmax(model.predict(Xtest), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3aa95de6e71b0b08d9671e094b40b291553b4e26ef8c06d5a593e09fce1813d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
